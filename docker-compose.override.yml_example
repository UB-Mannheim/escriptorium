version: "3.9"

services:
    web: &web
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == frontend0

    channelserver:
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == frontend0

    db:
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == db0

    nginx:
      restart: always
      ports:
       - "80:80"
      # - "443:443"
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == frontend0

      ### To enable SSL, generate keys (eg with letsencrypt/certbot)
      ### copy nginx/ssl_certificates.conf_example and edit it
      ## if need be to correspond to the volume below
      ### and uncomment this block and the port 443
      # volumes:
      #   - ${PWD}/nginx/ssl.conf:/etc/nginx/conf.d/nginx.conf
      #   - ${PWD}/nginx/ssl_certificates.conf:/etc/nginx/conf.d/ssl_certificates.conf
      #   - ${PWD}/nginx/certs/:/etc/certs/

    flower:
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == frontend0

    # cpus and mem_limit imposes a hard limit on cpus usage,
    # needed to keep some for http/db when working with a single machine
    #
    # according to the docker documentation:
    # Memory reservation is a kind of memory soft limit that allows for greater sharing of memory.
    # Under normal circumstances, containers can use as much of the memory as needed and are constrained only by the hard limits set with the -m/--memory option (mem_limit in docker-compose).
    # When memory reservation is set, Docker detects memory contention or low memory and forces containers to restrict their consumption to a reservation limit.

    # the shm_size argument is needed when using KRAKEN_TRAINING_LOAD_THREADS

    # !!!!
    # example values here are given for 16 cores and 16g of memory keeping 2 cores and 1g of memory at all time
    # for http & db assuming a low amount of concurent users <50

    celery-main:
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == compute0
      #   resources:
      #     limits:
      #       cpus: '6'
      #       memory: 15g
      #     reservations:
      #       memory: 4g

    celery-low-priority:
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.hostname == compute1
      #   resources:
      #     limits:
      #       cpus: '2'
      #       memory: 15g
      #     reservations:
      #       memory: 1g

    celery-gpu:
      restart: always
      # deploy:
      #   mode: replicated
      #   replicas: 1
      #   placement:
      #     constraints:
      #       - node.labels.type == gpu
      #   resources:
      #     limits:
      #       cpus: '6'
      #       memory: 15g
      #     reservations:
      #       memory: 1g

      # shm_size: '3gb'
      # runtime: nvidia
      # environment:
      #   - KRAKEN_TRAINING_DEVICE=cuda:0
      #   - NVIDIA_VISIBLE_DEVICES=all
      #   - NVIDIA_DRIVER_CAPABILITIES=all

    # add more workers for every physical gpu
    # celery-gpu2:
    #    <<: *celery-gpu
    #    environment:
    #       - KRAKEN_TRAINING_DEVICE=cuda:1

  # celerybeat:
     #   restart: always

#volumes:
#   static:
#    driver: local
#    driver_opts:
#      type: none
#      o: 'bind'
#      device: $PWD/static/
#
#   media:
#    driver: local
#    driver_opts:
#      type: none
#      o: 'bind'
#      device: $PWD/media/
